{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " ##%config Completer.use_jedi = False \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import sklearn as skl\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.linear_model as skl_lin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "import sklearn.mixture as skl_mix\n",
    "import copy\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "def matrix_to_vect(mnist_digits):\n",
    "    return np.reshape(mnist_digits, (-1, 784))\n",
    "\n",
    "## Load In Data ##\n",
    "(X_train_28x28, y_train), (X_test_28x28, y_test) = mnist.load_data()\n",
    "X_train_28x28 = X_train_28x28.astype('float32') / 255.\n",
    "X_test_28x28 = X_test_28x28.astype('float32') / 255.\n",
    "## Standardize Features to Standard Gaussians, on the flattened vector ##\n",
    "X_train = matrix_to_vect(X_train_28x28)\n",
    "X_test = matrix_to_vect(X_test_28x28)\n",
    "\n",
    "scaler = skl_pre.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Note we use the same transformation on the test set.\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "X_train_28x28x1 = X_train_28x28[..., None]\n",
    "X_test_28x28x1 = X_test_28x28[..., None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data split: For each model m_i with a corresponding cluster c_i, we first give it all the data in c_i, then bag/resample the remaining, 80% data from main clustering, 20% from all clusters (inclusive).\n",
    "\n",
    "Idea: Resample and train using the generated gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_resampled = []\n",
    "Y_data_resampled = []\n",
    "X_range = np.arange(0, X_train.shape[0])\n",
    "k_models = 15\n",
    "for i in range(k_models):\n",
    "    idx = np.random.choice(X_range,X_train.shape[0])\n",
    "    X_i = X_train_28x28x1[idx]\n",
    "    Y_i = y_train[idx]\n",
    "    X_data_resampled.append(X_i)\n",
    "    Y_data_resampled.append(Y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do LeNet-5 Architecture for EACH data set\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "## LeNet-5 Architecture keras code is taken from RPI FALL 2020 CSCI 4961 - Machine Learning & Optimization notes, by Prof. Alex Gittens ##\n",
    "## Note that the Architecture itself is from \"Gradient Based Learning Applied to Document Recognition\", (LeCun et al., 1998)            ##\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "\n",
    "models = []\n",
    "for m in range(k_models):\n",
    "    lenet = Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1), activation='tanh', padding=\"same\", name=\"C1\"),\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(1,1), padding='valid', name=\"A1\"), # no padding before pooling,\n",
    "    Conv2D(16, kernel_size=(5,5), strides=(1,1), activation='tanh', name=\"C2\"), # by default padding is \"valid\",\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name=\"A2\"),\n",
    "    Conv2D(120, kernel_size=(5,5), strides=(1,1), activation='tanh', padding='valid', name=\"C3\"),\n",
    "    Flatten(name=\"F\"),\n",
    "    Dense(84, activation='tanh', name=\"D1\"),\n",
    "    Dense(10, activation='softmax', name=\"D2\")])\n",
    "    lenet.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    models.append(lenet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 33s 68ms/step - loss: 0.4263 - accuracy: 0.8703\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.1599 - accuracy: 0.95330s - loss: 0.1600 - accuracy\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.1143 - accuracy: 0.9668\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0895 - accuracy: 0.9726\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0677 - accuracy: 0.9804\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0614 - accuracy: 0.9817\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0519 - accuracy: 0.9845\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0507 - accuracy: 0.9845\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0448 - accuracy: 0.9855\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0406 - accuracy: 0.9875\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.4157 - accuracy: 0.8753\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.1601 - accuracy: 0.9538\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.1166 - accuracy: 0.9651\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0831 - accuracy: 0.9749\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.0677 - accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 33s 69ms/step - loss: 0.0666 - accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0525 - accuracy: 0.9849\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0472 - accuracy: 0.9859\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0452 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0398 - accuracy: 0.9880\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 33s 69ms/step - loss: 0.3967 - accuracy: 0.8812\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.1580 - accuracy: 0.9542\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.1105 - accuracy: 0.9673\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0832 - accuracy: 0.9753\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0603 - accuracy: 0.9825\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0530 - accuracy: 0.9842\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0449 - accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0438 - accuracy: 0.9866\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0415 - accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0355 - accuracy: 0.9893\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 38s 78ms/step - loss: 0.4106 - accuracy: 0.8758\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 33s 69ms/step - loss: 0.1484 - accuracy: 0.9573\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0947 - accuracy: 0.9719\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0682 - accuracy: 0.9795\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0531 - accuracy: 0.9845\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0485 - accuracy: 0.9853\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0419 - accuracy: 0.9872\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0420 - accuracy: 0.9877\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0368 - accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0325 - accuracy: 0.9900\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 29s 59ms/step - loss: 0.4115 - accuracy: 0.8728\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.1660 - accuracy: 0.9528\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.1244 - accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0974 - accuracy: 0.9711\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0776 - accuracy: 0.9773\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0620 - accuracy: 0.9820\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0560 - accuracy: 0.9838\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.0514 - accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0436 - accuracy: 0.9868\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0457 - accuracy: 0.9863\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.4490 - accuracy: 0.8643\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.1695 - accuracy: 0.9488\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.1252 - accuracy: 0.9624\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0964 - accuracy: 0.9714\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0761 - accuracy: 0.97720s - loss: 0.0761 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0649 - accuracy: 0.9797\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 32s 69ms/step - loss: 0.0591 - accuracy: 0.9822\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 61ms/step - loss: 0.0448 - accuracy: 0.98660s - loss: 0.0448 - accuracy: \n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0413 - accuracy: 0.9880\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0428 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 30s 62ms/step - loss: 0.4262 - accuracy: 0.8712\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.1523 - accuracy: 0.9557\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0977 - accuracy: 0.9707\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0743 - accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0633 - accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0551 - accuracy: 0.98300s - loss: 0.0551 - accuracy: \n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0495 - accuracy: 0.9853\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.0463 - accuracy: 0.9854\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0440 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.0380 - accuracy: 0.9880\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.4044 - accuracy: 0.8784\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.1577 - accuracy: 0.95521s - loss: - ETA: 0s - loss: 0.1578 - accuracy: 0.95 - ETA: 0s - loss: 0.1577 - accuracy: \n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.1107 - accuracy: 0.9679\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0817 - accuracy: 0.9759\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0596 - accuracy: 0.9823\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0529 - accuracy: 0.9849\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0497 - accuracy: 0.9855\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0432 - accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.0406 - accuracy: 0.9880\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0385 - accuracy: 0.9884\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 36s 75ms/step - loss: 0.4171 - accuracy: 0.8729\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.1530 - accuracy: 0.9569\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.1128 - accuracy: 0.9663\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0813 - accuracy: 0.9764\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0627 - accuracy: 0.9814\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0545 - accuracy: 0.9826\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0521 - accuracy: 0.9846\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0462 - accuracy: 0.98680s - loss: 0.0462 - accuracy: 0.98\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0430 - accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0400 - accuracy: 0.9876\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 34s 67ms/step - loss: 0.4163 - accuracy: 0.8776\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.1548 - accuracy: 0.9544\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.1140 - accuracy: 0.9660\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0858 - accuracy: 0.9747\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0656 - accuracy: 0.9806\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.0566 - accuracy: 0.9828\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0531 - accuracy: 0.9843\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.0456 - accuracy: 0.9862\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.0440 - accuracy: 0.9863\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0360 - accuracy: 0.9894\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 33s 69ms/step - loss: 0.4174 - accuracy: 0.8759\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.1526 - accuracy: 0.9567\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.1035 - accuracy: 0.9701\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0810 - accuracy: 0.9754\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0603 - accuracy: 0.9828\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 38s 80ms/step - loss: 0.0569 - accuracy: 0.9831\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0519 - accuracy: 0.9838\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0427 - accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0415 - accuracy: 0.9879\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0423 - accuracy: 0.9863\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 31s 64ms/step - loss: 0.4359 - accuracy: 0.8690\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.1584 - accuracy: 0.95590s - loss: 0.1584 - accuracy: 0.95\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.1076 - accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0734 - accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.0626 - accuracy: 0.98131s - loss: 0 - ETA: 0s - loss: 0.0626 - accu\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0592 - accuracy: 0.9829\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0498 - accuracy: 0.9860\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0462 - accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0443 - accuracy: 0.9881\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.0377 - accuracy: 0.9891\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 31s 64ms/step - loss: 0.4268 - accuracy: 0.8673\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.1502 - accuracy: 0.9548\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.1098 - accuracy: 0.9684\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0761 - accuracy: 0.9780\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0628 - accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0533 - accuracy: 0.9847\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0505 - accuracy: 0.9846\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0446 - accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.0381 - accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0382 - accuracy: 0.9891\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.4270 - accuracy: 0.8727\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.1615 - accuracy: 0.9532\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.1122 - accuracy: 0.9671\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0808 - accuracy: 0.9759\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0682 - accuracy: 0.9805\n",
      "Epoch 6/10\n",
      "  6/469 [..............................] - ETA: 31s - loss: 0.0332 - accuracy: 0.9875"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for m in range(k_models):\n",
    "    history = (models[m]).fit(X_data_resampled[m], Y_data_resampled[m], epochs=10, batch_size=128, verbose=1)\n",
    "    hist.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_pred = np.zeros([X_test.shape[0], 10]) ## 10 for 10 classes in fashion mnist\n",
    "for m in range(k_models):\n",
    "    test_pred += (models[m]).predict(X_test_28x28x1) ## 10000 x 10?\n",
    "test_pred /= X_test.shape[0]\n",
    "Y_test_pred = np.argmax(test_pred, axis = 1) ## get the class!\n",
    "error = np.mean(np.where(y_test - Y_test_pred != 0, 1, 0)) ## different classes\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
