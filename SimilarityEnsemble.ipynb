{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "## %config Completer.use_jedi = False \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import sklearn as skl\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.linear_model as skl_lin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "import sklearn.mixture as skl_mix\n",
    "import copy\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "def matrix_to_vect(mnist_digits):\n",
    "    return np.reshape(mnist_digits, (-1, 784))\n",
    "\n",
    "## Load In Data ##\n",
    "(X_train_28x28, y_train), (X_test_28x28, y_test) = mnist.load_data()\n",
    "X_train_28x28 = X_train_28x28.astype('float32') / 255.\n",
    "X_test_28x28 = X_test_28x28.astype('float32') / 255.\n",
    "## Standardize Features to Standard Gaussians, on the flattened vector ##\n",
    "X_train = matrix_to_vect(X_train_28x28)\n",
    "X_test = matrix_to_vect(X_test_28x28)\n",
    "\n",
    "scaler = skl_pre.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Note we use the same transformation on the test set.\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "X_train_28x28x1 = X_train_28x28[..., None]\n",
    "X_test_28x28x1 = X_test_28x28[..., None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNUSED CODE ##\n",
    "## My original intention was to initialize centers by Lloyd's using a distance metric that encourages evenly separated clusters,\n",
    "## then apply X-means, then pass into an EM algorithm.\n",
    "## It turns out, scikit-learn's GMM model does k-means with random initialization, then applies their optimized, built in EM algorithm.\n",
    "\n",
    "\n",
    "n = X_train.shape[1]\n",
    "d = X_train.shape[0]\n",
    "## greedily compute the next k-1 centers furthest centers given a starting center mu, init\n",
    "## the data X, and k-clusters desired\n",
    "\n",
    "## distance measures to separate clusters\n",
    "## Try using sqrt of L2\n",
    "def dist(x1, x2):\n",
    "    return np.sqrt(np.linalg.norm(x1 - x2))\n",
    "## just use built in GMM model\n",
    "## We hope to maximize the iterative averaged distance\n",
    "def furthest_centers(mu, init, X, k):\n",
    "    np.random.seed(1999) ## \n",
    "    init = np.random.randint(0,n) ## get random point in dataset\n",
    "    visited_idx = [init] ## keep track of indices\n",
    "    centers = [X[init]]\n",
    "    for j in range(k-1):\n",
    "        for i in range(n):\n",
    "            if i in visited_idx:\n",
    "                continue ## one downside is that the norm only interprets total difference of pixels, and so the clusterings may be poor.\n",
    "            currmax = float('-inf')\n",
    "            avg_dist = 0\n",
    "            for c_j in centers: ## can we come up with a measure that encourages points far from all centers?\n",
    "                avg_dist += dist(c_j, X[i])\n",
    "            avg_dist /= len(centers)\n",
    "            if avg_dist > currmax:\n",
    "                currmax = avg_dist\n",
    "                maxidx = i\n",
    "        centers.append(X[maxidx])\n",
    "        visited_idx.append(maxidx)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass covariance_type=full as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 30\n",
      "  Iteration 40\n",
      "  Iteration 50\n",
      "  Iteration 60\n",
      "  Iteration 70\n",
      "Initialization converged: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=15, random_state=1999, verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_clusters = 15 ## do more clusters than likely necessary.\n",
    "## Cluster by k-means\n",
    "GMM = skl_mix.GaussianMixture(k_clusters, 'full', random_state = 1999, verbose = True)\n",
    "GMM.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data split: For each model m_i with a corresponding cluster c_i, we first give it all the data in c_i, then bag/resample the remaining, 80% data from main clustering, 20% from all clusters (inclusive).\n",
    "\n",
    "Idea: Resample and train using the generated gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusterings = [] ## store the indices of the clusterings\n",
    "pred = GMM.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_clusters = []\n",
    "Y_data_clusters = []\n",
    "for i in range(k_clusters):\n",
    "    Clusterings.append(np.where(pred == i))\n",
    "    remaining = X_train.shape[0] - Clusterings[0][0].shape[0]\n",
    "    main_cluster_int = int(np.floor(0.8*(remaining)))\n",
    "    outer_cluster_int = remaining - main_cluster_int\n",
    "    main = np.random.choice(Clusterings[0][0],main_cluster_int)\n",
    "    outer = np.random.choice(np.arange(0, X_train.shape[0]), outer_cluster_int)\n",
    "    X_i = np.concatenate([X_train_28x28[Clusterings[0][0]],X_train_28x28[main], X_train_28x28[outer]], axis = 0)\n",
    "    Y_i = np.concatenate([y_train[Clusterings[0][0]],y_train[main], y_train[outer]], axis = 0)\n",
    "    X_i = X_i[...,None]\n",
    "    X_data_clusters.append(X_i)\n",
    "    Y_data_clusters.append(Y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do LeNet-5 Architecture for EACH data set\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "## LeNet-5 Architecture keras code is taken from RPI FALL 2020 CSCI 4961 - Machine Learning & Optimization notes, by Prof. Alex Gittens ##\n",
    "## Note that the Architecture itself is from \"Gradient Based Learning Applied to Document Recognition\", (LeCun et al., 1998)            ##\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "\n",
    "models = []\n",
    "for m in range(k_clusters):\n",
    "    lenet = Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1), activation='tanh', padding=\"same\", name=\"C1\"),\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(1,1), padding='valid', name=\"A1\"), # no padding before pooling,\n",
    "    Conv2D(16, kernel_size=(5,5), strides=(1,1), activation='tanh', name=\"C2\"), # by default padding is \"valid\",\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name=\"A2\"),\n",
    "    Conv2D(120, kernel_size=(5,5), strides=(1,1), activation='tanh', padding='valid', name=\"C3\"),\n",
    "    Flatten(name=\"F\"),\n",
    "    Dense(84, activation='tanh', name=\"D1\"),\n",
    "    Dense(10, activation='softmax', name=\"D2\")])\n",
    "    lenet.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    models.append(lenet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 27s 56ms/step - loss: 0.2878 - accuracy: 0.9130\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 27s 57ms/step - loss: 0.0660 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 57ms/step - loss: 0.0504 - accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 57ms/step - loss: 0.0367 - accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 57ms/step - loss: 0.0299 - accuracy: 0.9921\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0245 - accuracy: 0.9930\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0222 - accuracy: 0.9937\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0201 - accuracy: 0.9945\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0198 - accuracy: 0.9945\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0173 - accuracy: 0.9948\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2830 - accuracy: 0.9184\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0631 - accuracy: 0.9848\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0465 - accuracy: 0.9882\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0384 - accuracy: 0.9898\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - 28s 59ms/step - loss: 0.0313 - accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0259 - accuracy: 0.9928\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0229 - accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0217 - accuracy: 0.9941\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0178 - accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0172 - accuracy: 0.9948\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 58ms/step - loss: 0.2854 - accuracy: 0.9165\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0647 - accuracy: 0.9840\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0439 - accuracy: 0.9887\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0368 - accuracy: 0.9905\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0311 - accuracy: 0.9914\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0292 - accuracy: 0.99240s - loss: 0.0292 - ac\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0246 - accuracy: 0.9929\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0205 - accuracy: 0.9947\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0207 - accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0177 - accuracy: 0.9953\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 29s 60ms/step - loss: 0.2845 - accuracy: 0.9154\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0652 - accuracy: 0.9838\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0485 - accuracy: 0.9874\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0385 - accuracy: 0.9898\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0360 - accuracy: 0.9906\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0294 - accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0307 - accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0252 - accuracy: 0.9923\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0257 - accuracy: 0.9929\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0202 - accuracy: 0.9943\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2989 - accuracy: 0.9089\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0639 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0541 - accuracy: 0.9852\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0378 - accuracy: 0.9905\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 61ms/step - loss: 0.0344 - accuracy: 0.9905\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 32s 67ms/step - loss: 0.0315 - accuracy: 0.9910\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0251 - accuracy: 0.9932\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0218 - accuracy: 0.9940\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0199 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0220 - accuracy: 0.9939\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2789 - accuracy: 0.9137\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0660 - accuracy: 0.9838\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0483 - accuracy: 0.9882\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0400 - accuracy: 0.9894\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0374 - accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0327 - accuracy: 0.9912\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0249 - accuracy: 0.9931\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0210 - accuracy: 0.9941\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0248 - accuracy: 0.9931\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0189 - accuracy: 0.9951\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.3063 - accuracy: 0.9118\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0640 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0482 - accuracy: 0.9882\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0378 - accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0352 - accuracy: 0.9906\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0305 - accuracy: 0.9914\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0235 - accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0221 - accuracy: 0.9941\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0248 - accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0194 - accuracy: 0.9944\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2861 - accuracy: 0.9160\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0645 - accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0459 - accuracy: 0.9886\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0382 - accuracy: 0.9894\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0312 - accuracy: 0.9919\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0277 - accuracy: 0.9925\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0270 - accuracy: 0.9925\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0230 - accuracy: 0.9932\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0233 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0200 - accuracy: 0.9942\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2870 - accuracy: 0.9202\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0641 - accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0493 - accuracy: 0.98790s - loss: 0\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0421 - accuracy: 0.9894\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0382 - accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0303 - accuracy: 0.9920\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0258 - accuracy: 0.9937\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0252 - accuracy: 0.9928\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0225 - accuracy: 0.9941\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0198 - accuracy: 0.9944\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2867 - accuracy: 0.9136\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0605 - accuracy: 0.9852\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0446 - accuracy: 0.9881\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0350 - accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0318 - accuracy: 0.9919\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0217 - accuracy: 0.9937\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0214 - accuracy: 0.9945\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0189 - accuracy: 0.9946\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0197 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0194 - accuracy: 0.9947\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2997 - accuracy: 0.9102\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0617 - accuracy: 0.9847\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0421 - accuracy: 0.9896\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0371 - accuracy: 0.9901\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0318 - accuracy: 0.9914\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0259 - accuracy: 0.9932\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0220 - accuracy: 0.9942\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0186 - accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0196 - accuracy: 0.9946\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0172 - accuracy: 0.9951\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2764 - accuracy: 0.9199\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0626 - accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0468 - accuracy: 0.9879\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0364 - accuracy: 0.9904\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0325 - accuracy: 0.9916\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0289 - accuracy: 0.9923\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0217 - accuracy: 0.9936\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0237 - accuracy: 0.9934\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0189 - accuracy: 0.9943\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0203 - accuracy: 0.9941\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.3015 - accuracy: 0.9077\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0633 - accuracy: 0.9854\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0502 - accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0380 - accuracy: 0.9899\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0351 - accuracy: 0.9903\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0311 - accuracy: 0.9913\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0274 - accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0260 - accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0218 - accuracy: 0.9938\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0264 - accuracy: 0.9924\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.2803 - accuracy: 0.9176\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0622 - accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 61ms/step - loss: 0.0468 - accuracy: 0.9884\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0399 - accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0318 - accuracy: 0.9919\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0283 - accuracy: 0.9926\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0252 - accuracy: 0.9937\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0233 - accuracy: 0.9939\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0229 - accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0187 - accuracy: 0.9952\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2845 - accuracy: 0.9156\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0609 - accuracy: 0.9852\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0497 - accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0392 - accuracy: 0.9904\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 0.0327 - accuracy: 0.9910\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0309 - accuracy: 0.9918\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0259 - accuracy: 0.9929\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0236 - accuracy: 0.9935\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0219 - accuracy: 0.9939\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0190 - accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for m in range(k_clusters):\n",
    "    history = (models[m]).fit(X_data_clusters[m], Y_data_clusters[m], epochs=10, batch_size=128, verbose=1)\n",
    "    hist.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_pred = np.zeros([X_test.shape[0], 10]) ## 10 for 10 classes in fashion mnist\n",
    "GMM_test = GMM.predict_proba(X_test) ## returns 10,000 x 15\n",
    "for m in range(k_clusters):\n",
    "    test_probabilities = (models[m]).predict(X_test_28x28x1) ## 10000 x 10?\n",
    "    test_pred += np.diag(GMM_test[:,m])@test_probabilities ## scale our predictions by our gaussians\n",
    "Y_test_pred = np.argmax(test_pred, axis = 1) ## get the class!\n",
    "error = np.mean(np.where(y_test - Y_test_pred != 0, 1, 0)) ## different classes\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.4182 - accuracy: 0.8716\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.1672 - accuracy: 0.9516\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.1202 - accuracy: 0.9654\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0938 - accuracy: 0.9717\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0833 - accuracy: 0.9747\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0727 - accuracy: 0.9788\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0684 - accuracy: 0.9793\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0594 - accuracy: 0.9822\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0588 - accuracy: 0.9819\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0515 - accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "lenet_full = Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    # padding=\"same\" pads input with enough zeros in each direction before convolution that the result of the convolution has the same size as the input image\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1), activation='tanh', padding=\"same\", name=\"C1\"),\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(1,1), padding='valid', name=\"A1\"), # no padding before pooling,\n",
    "    Conv2D(16, kernel_size=(5,5), strides=(1,1), activation='tanh', name=\"C2\"), # by default padding is \"valid\",\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name=\"A2\"),\n",
    "    Conv2D(120, kernel_size=(5,5), strides=(1,1), activation='tanh', padding='valid', name=\"C3\"),\n",
    "    Flatten(name=\"F\"),\n",
    "    Dense(84, activation='tanh', name=\"D1\"),\n",
    "    Dense(10, activation='softmax', name=\"D2\")\n",
    " ])\n",
    "lenet_full.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "history = lenet_full.fit(X_train_28x28x1, y_train, epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0184\n"
     ]
    }
   ],
   "source": [
    "test_probabilities = lenet_full.predict(X_test_28x28x1) ## 10000 x 10?\n",
    "Y_test_pred = np.argmax(test_probabilities, axis = 1) ## get the class\n",
    "error = np.mean(np.where(y_test - Y_test_pred != 0, 1, 0)) ## different classes\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "Double check LeNet5 with the full data set. Accuracy seems significantly off.\n",
    "** COULD BE BECAUSE OF DATA SCALING TO STD GAUSSIANS.\n",
    "- Why do we get decreased accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
