{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## %config Completer.use_jedi = False \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import sklearn as skl\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.linear_model as skl_lin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "import sklearn.mixture as skl_mix\n",
    "from utils import mnist_reader\n",
    "from utils import helper\n",
    "import copy\n",
    "from keras.datasets import mnist\n",
    "from utils import helper\n",
    "\n",
    "## Load In Data ##\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "\n",
    "## Standardize Features to Standard Gaussians ##\n",
    "scaler = skl_pre.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Note we use the same transformation on the test set.\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_28x28 = helper.vector_to_matrix_mnist(X_train)\n",
    "X_test_28x28 = helper.vector_to_matrix_mnist(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNUSED CODE ##\n",
    "## My original intention was to initialize centers by Lloyd's using a distance metric that encourages evenly separated clusters,\n",
    "## then apply X-means, then pass into an EM algorithm.\n",
    "## It turns out, scikit-learn's GMM model does k-means with random initialization, then applies their optimized, built in EM algorithm.\n",
    "\n",
    "\n",
    "n = X_train.shape[1]\n",
    "d = X_train.shape[0]\n",
    "## greedily compute the next k-1 centers furthest centers given a starting center mu, init\n",
    "## the data X, and k-clusters desired\n",
    "\n",
    "## distance measures to separate clusters\n",
    "## Try using sqrt of L2\n",
    "def dist(x1, x2):\n",
    "    return np.sqrt(np.linalg.norm(x1 - x2))\n",
    "## just use built in GMM model\n",
    "## We hope to maximize the iterative averaged distance\n",
    "def furthest_centers(mu, init, X, k):\n",
    "    np.random.seed(1999) ## \n",
    "    init = np.random.randint(0,n) ## get random point in dataset\n",
    "    visited_idx = [init] ## keep track of indices\n",
    "    centers = [X[init]]\n",
    "    for j in range(k-1):\n",
    "        for i in range(n):\n",
    "            if i in visited_idx:\n",
    "                continue ## one downside is that the norm only interprets total difference of pixels, and so the clusterings may be poor.\n",
    "            currmax = float('-inf')\n",
    "            avg_dist = 0\n",
    "            for c_j in centers: ## can we come up with a measure that encourages points far from all centers?\n",
    "                avg_dist += dist(c_j, X[i])\n",
    "            avg_dist /= len(centers)\n",
    "            if avg_dist > currmax:\n",
    "                currmax = avg_dist\n",
    "                maxidx = i\n",
    "        centers.append(X[maxidx])\n",
    "        visited_idx.append(maxidx)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass covariance_type=full as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 10\n",
      "  Iteration 20\n",
      "  Iteration 30\n",
      "  Iteration 40\n",
      "Initialization converged: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=15, random_state=1999, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_clusters = 15 ## do more clusters than likely necessary.\n",
    "## Cluster by k-means\n",
    "GMM = skl_mix.GaussianMixture(k_clusters, 'full', random_state = 1999, verbose = True)\n",
    "GMM.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data split: For each model m_i with a corresponding cluster c_i, we first give it all the data in c_i, then bag/resample the remaining, 80% data from main clustering, 20% from all clusters (inclusive).\n",
    "\n",
    "Idea: Resample and train using the generated gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusterings = [] ## store the indices of the clusterings\n",
    "pred = GMM.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_clusters = []\n",
    "Y_data_clusters = []\n",
    "for i in range(k_clusters):\n",
    "    Clusterings.append(np.where(pred == i))\n",
    "    remaining = X_train.shape[0] - Clusterings[0][0].shape[0]\n",
    "    main_cluster_int = int(np.floor(0.8*(remaining)))\n",
    "    outer_cluster_int = remaining - main_cluster_int\n",
    "    main = np.random.choice(Clusterings[0][0],main_cluster_int)\n",
    "    outer = np.random.choice(np.arange(0, X_train.shape[0]), outer_cluster_int)\n",
    "    X_i = np.concatenate([X_train_28x28[Clusterings[0][0]],X_train_28x28[main], X_train_28x28[outer]], axis = 0)\n",
    "    Y_i = np.concatenate([y_train[Clusterings[0][0]],y_train[main], y_train[outer]], axis = 0)\n",
    "    X_i = X_i[...,None]\n",
    "    X_data_clusters.append(X_i)\n",
    "    Y_data_clusters.append(Y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do LeNet-5 Architecture for EACH data set\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "## LeNet-5 Architecture keras code is taken from RPI FALL 2020 CSCI 4961 - Machine Learning & Optimization notes, by Prof. Alex Gittens ##\n",
    "## Note that the Architecture itself is from \"Gradient Based Learning Applied to Document Recognition\", (LeCun et al., 1998)            ##\n",
    "##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, InputLayer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import np_utils\n",
    "\n",
    "models = []\n",
    "for m in range(k_clusters):\n",
    "    lenet = Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1), activation='tanh', padding=\"same\", name=\"C1\"),\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(1,1), padding='valid', name=\"A1\"), # no padding before pooling,\n",
    "    Conv2D(16, kernel_size=(5,5), strides=(1,1), activation='tanh', name=\"C2\"), # by default padding is \"valid\",\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name=\"A2\"),\n",
    "    Conv2D(120, kernel_size=(5,5), strides=(1,1), activation='tanh', padding='valid', name=\"C3\"),\n",
    "    Flatten(name=\"F\"),\n",
    "    Dense(84, activation='tanh', name=\"D1\"),\n",
    "    Dense(10, activation='softmax', name=\"D2\")])\n",
    "    lenet.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    models.append(lenet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 34s 71ms/step - loss: 0.2892 - accuracy: 0.9049\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.0946 - accuracy: 0.9678\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0825 - accuracy: 0.9700\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0776 - accuracy: 0.9723\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0737 - accuracy: 0.9739\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0696 - accuracy: 0.9747\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0681 - accuracy: 0.9749\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.0667 - accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0683 - accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 0.0656 - accuracy: 0.9768\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 31s 64ms/step - loss: 0.3047 - accuracy: 0.9004\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 30s 64ms/step - loss: 0.1008 - accuracy: 0.9658\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0835 - accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0772 - accuracy: 0.9735\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0685 - accuracy: 0.9769\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0684 - accuracy: 0.9761\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0691 - accuracy: 0.9755\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0696 - accuracy: 0.9761\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0719 - accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0681 - accuracy: 0.9764\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2920 - accuracy: 0.9038\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0957 - accuracy: 0.9680\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0798 - accuracy: 0.9725\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0719 - accuracy: 0.9746\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0681 - accuracy: 0.97560s - los\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0641 - accuracy: 0.9764\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0658 - accuracy: 0.9766\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0596 - accuracy: 0.9786\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0627 - accuracy: 0.9776\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0620 - accuracy: 0.9773\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.3398 - accuracy: 0.8942\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0962 - accuracy: 0.9680\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0799 - accuracy: 0.9720\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0702 - accuracy: 0.9762\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0669 - accuracy: 0.9762\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0697 - accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0657 - accuracy: 0.9765\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 28s 60ms/step - loss: 0.0654 - accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0627 - accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0631 - accuracy: 0.9776\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 58ms/step - loss: 0.2762 - accuracy: 0.9069\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0970 - accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0786 - accuracy: 0.9739\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0758 - accuracy: 0.9749\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0762 - accuracy: 0.9732\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.0709 - accuracy: 0.9749\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0713 - accuracy: 0.9752\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0708 - accuracy: 0.9745\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 0.0690 - accuracy: 0.9751\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 0.0657 - accuracy: 0.9772\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 0.2890 - accuracy: 0.9055\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 30s 65ms/step - loss: 0.0979 - accuracy: 0.9674\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0890 - accuracy: 0.9698\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0787 - accuracy: 0.9724\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0749 - accuracy: 0.9739\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0782 - accuracy: 0.97220s - loss: 0.0782 - accuracy: \n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0724 - accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.0743 - accuracy: 0.9748\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.0727 - accuracy: 0.9747\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0726 - accuracy: 0.9744\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 38s 79ms/step - loss: 0.2824 - accuracy: 0.9035\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0909 - accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0812 - accuracy: 0.97151s\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 37s 79ms/step - loss: 0.0746 - accuracy: 0.9734\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 37s 79ms/step - loss: 0.0720 - accuracy: 0.9747\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0753 - accuracy: 0.9729\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0619 - accuracy: 0.9768\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0640 - accuracy: 0.9771\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 76ms/step - loss: 0.0654 - accuracy: 0.9765\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.0666 - accuracy: 0.97630s - loss: 0\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.2718 - accuracy: 0.9121\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 37s 79ms/step - loss: 0.1004 - accuracy: 0.9658\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.0830 - accuracy: 0.9707\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0835 - accuracy: 0.9710\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0775 - accuracy: 0.9711\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 36s 78ms/step - loss: 0.0754 - accuracy: 0.9726\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.0739 - accuracy: 0.9745\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 37s 79ms/step - loss: 0.0701 - accuracy: 0.9744\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 37s 79ms/step - loss: 0.0699 - accuracy: 0.9750\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0734 - accuracy: 0.9734\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.3144 - accuracy: 0.9035\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.1007 - accuracy: 0.9662\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0834 - accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.0744 - accuracy: 0.9739\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0705 - accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 38s 80ms/step - loss: 0.0663 - accuracy: 0.9771\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0655 - accuracy: 0.9766\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.0625 - accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0627 - accuracy: 0.9781\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0637 - accuracy: 0.9770\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 37s 75ms/step - loss: 0.2938 - accuracy: 0.9076\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.1000 - accuracy: 0.9648\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0835 - accuracy: 0.9717\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0783 - accuracy: 0.9728\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0717 - accuracy: 0.9755\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0733 - accuracy: 0.9738\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0705 - accuracy: 0.9745\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0718 - accuracy: 0.9747\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.0723 - accuracy: 0.97431s - l\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0713 - accuracy: 0.9744\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.3227 - accuracy: 0.9011\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0973 - accuracy: 0.9673\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.0871 - accuracy: 0.9701\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0812 - accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 71ms/step - loss: 0.0742 - accuracy: 0.9748\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0717 - accuracy: 0.9745\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 0.0708 - accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0714 - accuracy: 0.9749\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.0668 - accuracy: 0.9768\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.0696 - accuracy: 0.9742\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 37s 77ms/step - loss: 0.2967 - accuracy: 0.9015\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0994 - accuracy: 0.9672\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0870 - accuracy: 0.9702\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0797 - accuracy: 0.9708\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 33s 70ms/step - loss: 0.0773 - accuracy: 0.9722\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.0738 - accuracy: 0.9737\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 31s 66ms/step - loss: 0.0721 - accuracy: 0.9747\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 31s 67ms/step - loss: 0.0737 - accuracy: 0.9736\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0724 - accuracy: 0.9745\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.0674 - accuracy: 0.9756\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.3098 - accuracy: 0.9008\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.0993 - accuracy: 0.9665\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.0828 - accuracy: 0.9716\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 38s 80ms/step - loss: 0.0747 - accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0734 - accuracy: 0.9740\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0697 - accuracy: 0.9757\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 38s 80ms/step - loss: 0.0689 - accuracy: 0.9752\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.0703 - accuracy: 0.9756\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0697 - accuracy: 0.9746\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.0660 - accuracy: 0.9766\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 38s 79ms/step - loss: 0.3783 - accuracy: 0.8860\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0985 - accuracy: 0.9664\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0845 - accuracy: 0.9702\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.0752 - accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 39s 82ms/step - loss: 0.0730 - accuracy: 0.9733\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 0.0695 - accuracy: 0.9761\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 36s 78ms/step - loss: 0.0649 - accuracy: 0.9770\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 38s 80ms/step - loss: 0.0643 - accuracy: 0.9772\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.0674 - accuracy: 0.9756\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0634 - accuracy: 0.9771\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 39s 81ms/step - loss: 0.2818 - accuracy: 0.9045\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0958 - accuracy: 0.9661\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.0832 - accuracy: 0.9718\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0777 - accuracy: 0.9717\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 36s 78ms/step - loss: 0.0756 - accuracy: 0.9734\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.0694 - accuracy: 0.9752\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.0684 - accuracy: 0.9755\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.0704 - accuracy: 0.9744\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0666 - accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.0656 - accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "hist = []\n",
    "for m in range(k_clusters):\n",
    "    history = (models[m]).fit(X_data_clusters[m], Y_data_clusters[m], epochs=10, batch_size=128, verbose=1)\n",
    "    hist.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1744\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test_28x28 = helper.vector_to_matrix_mnist(X_test)\n",
    "\n",
    "test_pred = np.zeros([X_test.shape[0], 10]) ## 10 for 10 classes in fashion mnist\n",
    "GMM_test = GMM.predict_proba(X_test) ## 10,000 x 15\n",
    "X_test_28x28 = X_test_28x28[..., None]\n",
    "for m in range(k_clusters):\n",
    "    test_probabilities = (models[m]).predict(X_test_28x28) ## 10000 x 10?\n",
    "    test_pred += np.diag(GMM_test[:,m])@test_probabilities ## scale our predictions by our gaussians\n",
    "Y_test_pred = np.argmax(test_pred, axis = 1) ## get the class!\n",
    "error = np.mean(np.where(y_test - Y_test_pred != 0, 1, 0)) ## different classes\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 36s 75ms/step - loss: 0.5864 - accuracy: 0.7873\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.3664 - accuracy: 0.8697\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.3426 - accuracy: 0.8777\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 34s 72ms/step - loss: 0.3330 - accuracy: 0.8794\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.3084 - accuracy: 0.8904\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 35s 74ms/step - loss: 0.3112 - accuracy: 0.8884\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.3032 - accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.3071 - accuracy: 0.8879\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 35s 75ms/step - loss: 0.2979 - accuracy: 0.8932\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 34s 73ms/step - loss: 0.2912 - accuracy: 0.8940\n"
     ]
    }
   ],
   "source": [
    "X_train_28x28x1 = X_train_28x28[...,None]\n",
    "lenet_full = Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    # padding=\"same\" pads input with enough zeros in each direction before convolution that the result of the convolution has the same size as the input image\n",
    "    Conv2D(6, kernel_size=(5,5), strides=(1,1), activation='tanh', padding=\"same\", name=\"C1\"),\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(1,1), padding='valid', name=\"A1\"), # no padding before pooling,\n",
    "    Conv2D(16, kernel_size=(5,5), strides=(1,1), activation='tanh', name=\"C2\"), # by default padding is \"valid\",\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name=\"A2\"),\n",
    "    Conv2D(120, kernel_size=(5,5), strides=(1,1), activation='tanh', padding='valid', name=\"C3\"),\n",
    "    Flatten(name=\"F\"),\n",
    "    Dense(84, activation='tanh', name=\"D1\"),\n",
    "    Dense(10, activation='softmax', name=\"D2\")\n",
    " ])\n",
    "lenet_full.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "history = lenet_full.fit(X_train_28x28x1, y_train, epochs=10, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1286\n"
     ]
    }
   ],
   "source": [
    "test_probabilities = lenet_full.predict(X_test_28x28) ## 10000 x 10?\n",
    "Y_test_pred = np.argmax(test_probabilities, axis = 1) ## get the class\n",
    "error = np.mean(np.where(y_test - Y_test_pred != 0, 1, 0)) ## different classes\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "Double check LeNet5 with the full data set. Accuracy seems significantly off.\n",
    "** COULD BE BECAUSE OF DATA SCALING TO STD GAUSSIANS.\n",
    "- Why do we get decreased accuracy? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
